{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Tokenize text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Tokenize each sentence into words\n",
        "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_sentences = []\n",
        "    for sentence_tokens in word_tokens:\n",
        "        filtered_sentence = [word for word in sentence_tokens if word.lower() not in stop_words]\n",
        "        filtered_sentences.append(filtered_sentence)\n",
        "\n",
        "    # Flatten the list of words into a single list of all words\n",
        "    all_words = [word for sentence in filtered_sentences for word in sentence]\n",
        "\n",
        "    # Convert the list of words back to text\n",
        "    processed_text = \" \".join(all_words)\n",
        "\n",
        "    return processed_text, sentences, word_tokens\n",
        "\n",
        "# Read text data from the .txt file\n",
        "def read_text_from_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "# Example .txt file path\n",
        "file_path = 'paper.txt'\n",
        "\n",
        "# Read text from the .txt file\n",
        "text = read_text_from_file(file_path)\n",
        "\n",
        "# Preprocess the text\n",
        "processed_text, sentences, word_tokens = preprocess_text(text)\n",
        "\n",
        "# Vectorize the preprocessed text\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorized_text = vectorizer.fit_transform([processed_text])\n",
        "\n",
        "# Calculate cosine similarity for each sentence\n",
        "sentence_cosine_sim = cosine_similarity(vectorized_text, vectorized_text)\n",
        "\n",
        "# Calculate cosine similarity for each word\n",
        "word_vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
        "word_vectorized_text = word_vectorizer.fit_transform(word_tokens)\n",
        "word_cosine_sim = cosine_similarity(word_vectorized_text, word_vectorized_text)\n",
        "\n",
        "print(\"Preprocessed text:\")\n",
        "print(processed_text)\n",
        "print(\"\\nVectorized text:\")\n",
        "print(vectorized_text)\n",
        "print(\"\\nCosine similarity matrix for each sentence:\")\n",
        "print(sentence_cosine_sim)\n",
        "print(\"\\nCosine similarity matrix for each word:\")\n",
        "print(word_cosine_sim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5XHg6sDwygf",
        "outputId": "e51c338d-ccef-47e2-f4ea-0ced35743777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed text:\n",
            "numbers satellite going launched next year decade decade place satellites [ 1 ] . Incidents based past data anti-satellite war head Chinese anti-satellite test , 2007 created debris cloud habitat ’ got learn number launches increase . Suddenly 2017-2019 ( 450-583 ) launches single year . , ’ defined things going dangerous . According defined law implemented till documentation . Collison operation not-operational satellite per report explained rapidly debris increasing . lead collision multi-artificial object space . ’ threat space environment , ’ something make detectable removal technology . [ 2 ] , [ 3 ] [ 4 ] . study [ 5 ] , [ 6 ] [ 7 ] describe hypervelocity many past collision incident happened . Hypervelocity impact small object easily defect destroy electrical system satellite . event related ISS ( International Space Station ) clearly shows dangerous . small size debris intimidating space environment . 2024 , reports many private government-based space agencies launching high rate satellite LEO ( Low Earth Orbit ) , GEO ( Geosynchronous Earth Orbit ) MEO ( Medium Earth Orbit ) . paper [ 8 ] , propose compliances rate different satellites space describing 10 % satellite satisfy mitigation rule UNSOOA . mitigation life time satellite 25 years . high chance propellent system spacecraft active availability fuel tank rocket boy . proper debris disposal non-operation satellite graveyard , 300 KM GEO . 10 % spacecraft satellite successfully reached graveyard orbit . CubeSats building companies designing debris mitigated methodologies satellite . Efficient selection removal target , critically , index debris objects [ 9 ] .\n",
            "\n",
            "Vectorized text:\n",
            "  (0, 96)\t0.04789131426105757\n",
            "  (0, 68)\t0.04789131426105757\n",
            "  (0, 27)\t0.04789131426105757\n",
            "  (0, 134)\t0.04789131426105757\n",
            "  (0, 120)\t0.04789131426105757\n",
            "  (0, 45)\t0.04789131426105757\n",
            "  (0, 86)\t0.04789131426105757\n",
            "  (0, 87)\t0.04789131426105757\n",
            "  (0, 37)\t0.04789131426105757\n",
            "  (0, 24)\t0.04789131426105757\n",
            "  (0, 17)\t0.04789131426105757\n",
            "  (0, 28)\t0.04789131426105757\n",
            "  (0, 110)\t0.04789131426105757\n",
            "  (0, 130)\t0.04789131426105757\n",
            "  (0, 72)\t0.04789131426105757\n",
            "  (0, 6)\t0.04789131426105757\n",
            "  (0, 56)\t0.09578262852211514\n",
            "  (0, 91)\t0.04789131426105757\n",
            "  (0, 41)\t0.04789131426105757\n",
            "  (0, 106)\t0.04789131426105757\n",
            "  (0, 16)\t0.04789131426105757\n",
            "  (0, 115)\t0.04789131426105757\n",
            "  (0, 133)\t0.04789131426105757\n",
            "  (0, 50)\t0.04789131426105757\n",
            "  (0, 14)\t0.04789131426105757\n",
            "  :\t:\n",
            "  (0, 78)\t0.04789131426105757\n",
            "  (0, 54)\t0.04789131426105757\n",
            "  (0, 57)\t0.04789131426105757\n",
            "  (0, 21)\t0.04789131426105757\n",
            "  (0, 31)\t0.2873478855663454\n",
            "  (0, 26)\t0.04789131426105757\n",
            "  (0, 1)\t0.04789131426105757\n",
            "  (0, 136)\t0.04789131426105757\n",
            "  (0, 19)\t0.04789131426105757\n",
            "  (0, 59)\t0.04789131426105757\n",
            "  (0, 142)\t0.04789131426105757\n",
            "  (0, 12)\t0.09578262852211514\n",
            "  (0, 30)\t0.04789131426105757\n",
            "  (0, 101)\t0.09578262852211514\n",
            "  (0, 15)\t0.09578262852211514\n",
            "  (0, 65)\t0.04789131426105757\n",
            "  (0, 118)\t0.09578262852211514\n",
            "  (0, 103)\t0.04789131426105757\n",
            "  (0, 32)\t0.09578262852211514\n",
            "  (0, 143)\t0.09578262852211514\n",
            "  (0, 90)\t0.04789131426105757\n",
            "  (0, 73)\t0.04789131426105757\n",
            "  (0, 53)\t0.09578262852211514\n",
            "  (0, 117)\t0.5268044568716332\n",
            "  (0, 94)\t0.04789131426105757\n",
            "\n",
            "Cosine similarity matrix for each sentence:\n",
            "[[1.]]\n",
            "\n",
            "Cosine similarity matrix for each word:\n",
            "[[1.         0.05972527 0.08080402 0.08941471 0.01442138 0.08094694\n",
            "  0.09204031 0.0535688  0.17989255 0.17347647 0.07673947 0.02831989\n",
            "  0.11916817 0.07588848 0.16318948 0.084536   0.14338045 0.09209956\n",
            "  0.11632453 0.05074819 0.08830396]\n",
            " [0.05972527 1.         0.07660078 0.14808539 0.02981511 0.07534668\n",
            "  0.13734528 0.10006965 0.03656719 0.1225113  0.08529771 0.06425913\n",
            "  0.10319674 0.05686255 0.1369025  0.04823285 0.09235348 0.12647882\n",
            "  0.08479917 0.02841071 0.09978358]\n",
            " [0.08080402 0.07660078 1.         0.01036446 0.01043497 0.03766401\n",
            "  0.04950186 0.00685604 0.00875359 0.00638391 0.00910824 0.12053889\n",
            "  0.0117559  0.20892838 0.0297603  0.01209299 0.03304908 0.00843948\n",
            "  0.00999936 0.00945216 0.00905351]\n",
            " [0.08941471 0.14808539 0.01036446 1.         0.1021236  0.00807328\n",
            "  0.01061073 0.236588   0.03613252 0.06500216 0.00895455 0.17761857\n",
            "  0.01155754 0.0537392  0.02678326 0.01188895 0.04453546 0.03483594\n",
            "  0.00983064 0.05842024 0.06584023]\n",
            " [0.01442138 0.02981511 0.01043497 0.1021236  1.         0.01984592\n",
            "  0.05688473 0.0165693  0.00866444 0.02453765 0.03500908 0.05375956\n",
            "  0.09873075 0.01041503 0.05271641 0.08430595 0.10361771 0.08292063\n",
            "  0.03843426 0.00935589 0.02188   ]\n",
            " [0.08094694 0.07534668 0.03766401 0.00807328 0.01984592 1.\n",
            "  0.07491767 0.01303928 0.03127346 0.06002701 0.07568945 0.0586089\n",
            "  0.11983402 0.05505202 0.13088134 0.09647464 0.05001751 0.11653187\n",
            "  0.05515943 0.06479267 0.04966961]\n",
            " [0.09204031 0.13734528 0.04950186 0.01061073 0.05688473 0.07491767\n",
            "  1.         0.10288431 0.00896159 0.14914777 0.2962955  0.07398321\n",
            "  0.20513614 0.07194248 0.25124472 0.0920242  0.17620265 0.17391733\n",
            "  0.16178967 0.00967675 0.08843527]\n",
            " [0.0535688  0.10006965 0.00685604 0.236588   0.0165693  0.01303928\n",
            "  0.10288431 1.         0.02390151 0.02940133 0.02300187 0.06922564\n",
            "  0.19449728 0.03958856 0.07044193 0.01920201 0.02495261 0.06898993\n",
            "  0.05156665 0.00614706 0.10476412]\n",
            " [0.17989255 0.03656719 0.00875359 0.03613252 0.00866444 0.03127346\n",
            "  0.00896159 0.02390151 1.         0.54568829 0.03468723 0.00640296\n",
            "  0.00976125 0.05213707 0.21284173 0.01004114 0.00598305 0.02942168\n",
            "  0.00830275 0.00784839 0.29405996]\n",
            " [0.17347647 0.1225113  0.00638391 0.06500216 0.02453765 0.06002701\n",
            "  0.14914777 0.02940133 0.54568829 1.         0.1036142  0.04689033\n",
            "  0.11094308 0.07991915 0.26156914 0.09301072 0.09730006 0.11072643\n",
            "  0.06650306 0.00572375 0.27649477]\n",
            " [0.07673947 0.08529771 0.00910824 0.00895455 0.03500908 0.07568945\n",
            "  0.2962955  0.02300187 0.03468723 0.1036142  1.         0.02587146\n",
            "  0.20250064 0.06877456 0.19537174 0.11804437 0.1945695  0.12442664\n",
            "  0.12251614 0.03428711 0.09633704]\n",
            " [0.02831989 0.06425913 0.12053889 0.17761857 0.05375956 0.0586089\n",
            "  0.07398321 0.06922564 0.00640296 0.04689033 0.02587146 1.\n",
            "  0.07296132 0.14606267 0.07367864 0.06230149 0.12376993 0.14153656\n",
            "  0.05799983 0.00691394 0.01616916]\n",
            " [0.11916817 0.10319674 0.0117559  0.01155754 0.09873075 0.11983402\n",
            "  0.20513614 0.19449728 0.00976125 0.11094308 0.20250064 0.07296132\n",
            "  1.         0.05419864 0.19555904 0.20490099 0.20230537 0.24056996\n",
            "  0.09895669 0.05904206 0.1282287 ]\n",
            " [0.07588848 0.05686255 0.20892838 0.0537392  0.01041503 0.05505202\n",
            "  0.07194248 0.03958856 0.05213707 0.07991915 0.06877456 0.14606267\n",
            "  0.05419864 1.         0.15406455 0.03830381 0.10837618 0.08964085\n",
            "  0.03756505 0.03665004 0.07199202]\n",
            " [0.16318948 0.1369025  0.0297603  0.02678326 0.05271641 0.13088134\n",
            "  0.25124472 0.07044193 0.21284173 0.26156914 0.19537174 0.07367864\n",
            "  0.19555904 0.15406455 1.         0.19862449 0.20039316 0.18359958\n",
            "  0.26370906 0.02442576 0.2107885 ]\n",
            " [0.084536   0.04823285 0.01209299 0.01188895 0.08430595 0.09647464\n",
            "  0.0920242  0.01920201 0.01004114 0.09301072 0.11804437 0.06230149\n",
            "  0.20490099 0.03830381 0.19862449 1.         0.17235932 0.19201784\n",
            "  0.10541141 0.04552294 0.06914585]\n",
            " [0.14338045 0.09235348 0.03304908 0.04453546 0.10361771 0.05001751\n",
            "  0.17620265 0.02495261 0.00598305 0.09730006 0.1945695  0.12376993\n",
            "  0.20230537 0.10837618 0.20039316 0.17235932 1.         0.17018074\n",
            "  0.22037781 0.04061537 0.11122635]\n",
            " [0.09209956 0.12647882 0.00843948 0.03483594 0.08292063 0.11653187\n",
            "  0.17391733 0.06898993 0.02942168 0.11072643 0.12442664 0.14153656\n",
            "  0.24056996 0.08964085 0.18359958 0.19201784 0.17018074 1.\n",
            "  0.2237959  0.06658874 0.1488669 ]\n",
            " [0.11632453 0.08479917 0.00999936 0.00983064 0.03843426 0.05515943\n",
            "  0.16178967 0.05156665 0.00830275 0.06650306 0.12251614 0.05799983\n",
            "  0.09895669 0.03756505 0.26370906 0.10541141 0.22037781 0.2237959\n",
            "  1.         0.03764166 0.06955421]\n",
            " [0.05074819 0.02841071 0.00945216 0.05842024 0.00935589 0.06479267\n",
            "  0.00967675 0.00614706 0.00784839 0.00572375 0.03428711 0.00691394\n",
            "  0.05904206 0.03665004 0.02442576 0.04552294 0.04061537 0.06658874\n",
            "  0.03764166 1.         0.04546973]\n",
            " [0.08830396 0.09978358 0.00905351 0.06584023 0.02188    0.04966961\n",
            "  0.08843527 0.10476412 0.29405996 0.27649477 0.09633704 0.01616916\n",
            "  0.1282287  0.07199202 0.2107885  0.06914585 0.11122635 0.1488669\n",
            "  0.06955421 0.04546973 1.        ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}